## Purpose

This folder defines an Acorn service which allows to create a Mongo Atlas cluster on the fly. 

In this very early version each cluster created by the service has the following characteristics:
- cloud provider: AWS 
- region: EU_WEST_1
- tier: M0

Notes: due to Atlas restriction, only one M0 cluster can be created in each Atlas project

## Prerequisites

To use this service you need to have an Atlas Mongo account, to create an organization and a project within this one.

Note: this example uses an organization named *Techwhale* containing the project *webhooks*

Next create a public / private api key pair at the organization level

![Organization api keys](./images/organization-api-keys.png)

Next get the project ID

![Getting project ID](./images/project-id.png)

For this demo I set those 3 values in the following environment variables:

- MONGODB_ATLAS_PUBLIC_API_KEY
- MONGODB_ATLAS_PRIVATE_API_KEY
- MONGODB_ATLAS_PROJECT_ID

We will use these environment variables in the next part.

## Definition of the service

The following Acornfile is the definition of the *atlas* service:

```
services: atlas: generated: job: "create-mongo-atlas-service"

jobs: "create-mongo-atlas-service": {
  image: "lucj/atlas-cli:v0.2.0"
  entrypoint: ["/bin/sh", "-c", "/render-service.sh"]
  env: {
    MONGODB_ATLAS_PUBLIC_API_KEY: "secret://atlas-creds/public_key"
    MONGODB_ATLAS_PRIVATE_API_KEY: "secret://atlas-creds/private_key"
    MONGODB_ATLAS_PROJECT_ID: "secret://atlas-creds/project_id"
    DB_USER: "secret://internal-db-creds/username"
    DB_PASS: "secret://internal-db-creds/password"
  }
  files: "/render-service.sh": """
    # Create an atlas cluster (using default properties for now)
    PROVIDER="AWS"
    REGION="EU_WEST_1"
    TIER="M0"

    # Create a cluster in the current project
    atlas cluster create test --region $REGION --provider $PROVIDER --tier $TIER

    # Wait for Atlas to provide cluster's connection string
    while true; do
      DB_ADDRESS=$(atlas cluster describe test -o json | jq -r .connectionStrings.standardSrv)
      echo ${DB_ADDRESS}
      if [ "${DB_ADDRESS}" = "null" ]; then
          sleep 2
      else
        break
      fi
    done

    # Allow database network access from current IP
    atlas accessList create --currentIp

    # Create db user
    atlas dbusers create atlasAdmin --username ${DB_USER} --password ${DB_PASS}

    # Extract proto and host from address returned
    DB_PROTO=$(echo $DB_ADDRESS | cut -d':' -f1)
    DB_HOST=$(echo $DB_ADDRESS | cut -d'/' -f3)
    echo "DB_ADDRESS: [${DB_ADDRESS}] / DB_PROTO:[${DB_PROTO}] / DB_HOST:[${DB_HOST}]"

    cat > /run/secrets/output<<EOF
    services: atlas: {
      address: "${DB_HOST}"
      secrets: ["db-creds"]
      data: {
        proto: "${DB_PROTO}"
      }
    }
    secrets: "db-creds": {
      type: "basic"
      data: {
        username: "${DB_USER}"
        password: "${DB_PASS}"
      }
    }
  EOF
  """
}

secrets: {
  "internal-db-creds": type: "basic"

  "db-creds": {
      type: "generated"
      params: job: "create-mongo-atlas-service"
  }

  "atlas-creds": {
      external: "atlas-creds"
      type: "opaque"
      data: {
          public_key: "MONGODB_ATLAS_PUBLIC_API_KEY"
          private_key: "MONGODB_ATLAS_PRIVATE_API_KEY"
          project_id: "MONGODB_ATLAS_PROJECT_ID"
      }
  }
}
```

This file contains 3 top level keys:
- service (*atlas*)
- job (*create-mongo-atlas-service*)
- secrets (*internal-db-creds*, *db-creds* and *atlas-creds*)

The *atlas* service is generated by the job *create-mongo-atlas-service*. This job contains all the logic to create an Atlas cluster, it uses the atlas cli binary present in the container image *lucj/atlas-cli:v0.2.0* for the creation of the cluster and of a first db user. The MongoDB connection string is generated using the db user and the database address. When called from another Acorn (as we will see below) a service will provide the MongoDB URI to the calling application.

The *internal-db-creds* secret is used to generated username / password on the file, those ones are provided to the job via environment variable so it can create db user.

The *db-creds* secret is generated by the job *create-mongo-atlas-service*, it contains the credentials of the db user.

The *atlas-creds* secret defines an external secret containing the credentials to connect to the Atlas account. This secret must exist in the Acorn project before the service can be used.

## Running the service

The service can be called directly from the above Acornfile.

First we need to create the secret *atlas-creds* providing the public and private keys as well as the Atlas project ID we want the MongoDB cluster to be created in.

Note: the following example uses environment varibales already defined in the current shell 

```
acorn secrets create \
  --type opaque \
  --data public_key=$MONGODB_ATLAS_PUBLIC_API_KEY \
  --data private_key=$MONGODB_ATLAS_PRIVATE_API_KEY \
  --data project_id=$MONGODB_ATLAS_PROJECT_ID \
  atlas-creds
```

Next we run the Acorn:

```
acorn run -n atlas .
```

In a few tens of seconds a new Atlas cluster will be up and running.

![Cluster created](./images/cluster-created.png)

Running the service directly was just a test to ensure a cluster is actually created from this service.

Then we can delete the application:

```
acorn rm atlas --all --force
```

Also, from the Atlas dashboard, we make sure to delete the cluster as we will create a new one in a next section.

![Cluster deletion](./images/cluster-deletion-1.png)

![Cluster deletion](./images/cluster-deletion-2.png)

## Publishing the service

The idea is not to run the service from its own Acornfile but to reference the service by its name from other Acorns. For this purpose we first need to build the image of the service (as we would do for a standard Acorn application):

```
acorn build -t docker.io/lucj/acorn-atlas-service:v0.6.0 .
```

Next we push the image to an OCI registry (Docker Hub in this example):

```
acorn push docker.io/lucj/acorn-atlas-service:v0.6.0
```

![Acorn image available in the Docker Hub](./images/dockerhub.png)

Once the image is in the registry it can be used by other applications.

## Using the service

As we have done when running the service from its own Acornfile, we need to create a secret containing the atlas api keys (this secret will allow the job to connect to the Atlas account).

Notes:
- the following example uses environment variables already defined in the current shell 
- if you have already created the secret in the previous step there is no need to run the command once again

```
acorn secrets create \
  --type opaque \
  --data public_key=$MONGODB_ATLAS_PUBLIC_API_KEY \
  --data private_key=$MONGODB_ATLAS_PRIVATE_API_KEY \
  --data project_id=$MONGODB_ATLAS_PROJECT_ID \
  atlas-creds
```

### Using the atlas service from a simple application

The following Acornfile defines 2 items:
- a reference to the *atlas* service
- a container named *app* using this service

The container only tries to connect to the database using the MONGODB_URL provided in the env variable, this one is created from the service's properties: 

```
services: atlas: {
    image: "docker.io/lucj/acorn-atlas-service:v0.6.0"
}

containers: app: {
  image: "mongo:6.0.5"
  entrypoint: ["/bin/sh", "-c", "/check-db.sh"]
  env: {
    MONGODB_URI: "@{services.atlas.data.proto}://@{services.atlas.secrets.db-creds.username}:@{services.atlas.secrets.db-creds.password}@@{services.atlas.address}"
  }
  files: "/check-db.sh": """
    echo "Will try to connect to [${MONGODB_URI}]"
    while true; do
      echo "=> testing DB connection..."
      mongosh --eval "db.adminCommand('ping')" ${MONGODB_URI} 
      if [ $? -eq 0 ]; then
        break
      else
        sleep 5
      fi
    done
    echo "connected to the DB"
    sleep 3600
  """
}
```

This simple application can be run with the following command:

```
acorn run -n app
```

From Atlas dashboard we could see a MongoDB cluster was automatically created.

Also, from the *app* container logs we can see the connection was successfull after the second attempt:

```
# acorn logs app
app-9b6876cbb-49x88: Will try to connect to [mongodb+srv://l4rcsrbw:wk9d5cgfkxf7hk8d@test.9flmpis.mongodb.net]
app-9b6876cbb-49x88: => testing DB connection...
app-9b6876cbb-49x88: Current Mongosh Log ID:    646a6d1bfaedda3a9caa08a1
app-9b6876cbb-49x88: Connecting to:             mongodb+srv://<credentials>@test.9flmpis.mongodb.net/?appName=mongosh+1.8.2
app-9b6876cbb-49x88: MongoServerError: bad auth : Authentication failed.
app-9b6876cbb-49x88: => testing DB connection...
app-9b6876cbb-49x88: Current Mongosh Log ID:    646a6d21847e03e28677ceab
app-9b6876cbb-49x88: Connecting to:             mongodb+srv://<credentials>@test.9flmpis.mongodb.net/?appName=mongosh+1.8.2
app-9b6876cbb-49x88: Using MongoDB:             6.0.6
app-9b6876cbb-49x88: Using Mongosh:             1.8.2
app-9b6876cbb-49x88: 
app-9b6876cbb-49x88: For mongosh info see: https://docs.mongodb.com/mongodb-shell/
app-9b6876cbb-49x88: 
app-9b6876cbb-49x88: 
app-9b6876cbb-49x88: To help improve our products, anonymous usage data is collected and sent to MongoDB periodically (https://www.mongodb.com/legal/privacy-policy).
app-9b6876cbb-49x88: You can opt-out by running the disableTelemetry() command.
app-9b6876cbb-49x88: 
app-9b6876cbb-49x88: { ok: 1 }
```

We've seen all to use a service right from a (simple) application container. In the next part we will use in a more interesting application. Before going to the next part, we delete the Acorn app and remove the Atlas cluster from the dashboard (as we've done previously).

### Using the atlas service in the Webhooks application

:warning: This is WIP, currently the webhook app cannot be fully deployed using the Atlas service.

#### About the webhooks application

[https://webhooks.app/](https://webhooks.app/) is a microservice application which purpose is to provide a webhook (HTTP POST endpoint) on the fly, it is mainly dedicated to tests and demos. The source code is available in GitLab [https://gitlab.com/web-hook](https://gitlab.com/web-hook), each time a change is done in one of the microservices the application is build and pushed to the Docker Hub as an Acorn image in [https://hub.docker.com/repository/docker/lucj/webhooksapp/general](https://hub.docker.com/repository/docker/lucj/webhooksapp/general).

#### Running the webhooks with Acorn

The Acornfile used to build this image is [https://gitlab.com/web-hook/config/-/blob/main/apps/acorn/Acornfile](https://gitlab.com/web-hook/config/-/blob/main/apps/acorn/Acornfile). 

In order to run the webhooks app as Acorn and use it with an auto-generated MongoDB Atlas cluster, we run the latest version of this Acorn image specifying the *--atlas* flag.

```
acorn run -n webhook lucj/webhooksapp --atlas
```

The Atlas cluster is correctly created (this can be verified from the Atlas dashboard), but the application never reached the ready state and remains in pending. Below are the logs provided after running the previous command:

```
webhook
STATUS: ENDPOINTS[] HEALTHY[] UPTODATE[] 
STATUS: ENDPOINTS[] HEALTHY[0] UPTODATE[0] [defined: waiting on missing secrets [atlas]]
STATUS: ENDPOINTS[] HEALTHY[0/1] UPTODATE[1] [defined: waiting on missing secrets [atlas]] [containers: api pending create; nats ContainerCreating; swagger pending create; wss pending create; www pending create]
STATUS: ENDPOINTS[http://default-webhook-a1a950fb.11jp51.alpha.on-acorn.io => default:8080] HEALTHY[0/1] UPTODATE[1] [defined: waiting on missing secrets [atlas]] [containers: api pending create; nats ContainerCreating; swagger pending create; wss pending create; www pending create]
STATUS: ENDPOINTS[http://default-webhook-a1a950fb.11jp51.alpha.on-acorn.io => default:8080] HEALTHY[0/1] UPTODATE[1] [defined: waiting on missing secrets [atlas]] [containers: api pending create; nats is not ready; swagger pending create; wss pending create; www pending create]
STATUS: ENDPOINTS[http://default-webhook-a1a950fb.11jp51.alpha.on-acorn.io => default:8080] HEALTHY[1] UPTODATE[1] [defined: waiting on missing secrets [atlas]]
STATUS: ENDPOINTS[http://default-webhook-a1a950fb.11jp51.alpha.on-acorn.io => default:8080] HEALTHY[1] UPTODATE[1] pending
```

We can get some more details:

```
# acorn all

APPS:
NAME            IMAGE                                       HEALTHY   UP-TO-DATE   CREATED   ENDPOINTS                                                                  MESSAGE
webhook.atlas   docker.io/lucj/acorn-atlas-service:v0.6.0   0         0            98s ago                                                                              OK
webhook         lucj/webhooksapp                            1         1            99s ago   http://default-webhook-a1a950fb.11jp51.alpha.on-acorn.io => default:8080   pending

CONTAINERS:
NAME                            APP       IMAGE             STATE     RESTARTCOUNT   CREATED   MESSAGE
webhook.nats-7bc944c795-kt8kx   webhook   nats:2.9-alpine   running   0              98s ago   

VOLUMES:
NAME      APP-NAME   BOUND-VOLUME   CAPACITY   VOLUME-CLASS   STATUS    ACCESS-MODES   CREATED

SECRETS:
NAME                              TYPE      KEYS                                  CREATED
webhook.atlas.db-creds            basic     [password username]                   71s ago
webhook.atlas.internal-db-creds   basic     [password username]                   97s ago
atlas-creds                       opaque    [private_key project_id public_key]   5d11h ago
```

Last log of the acorn controller:
```
time="2023-05-21T19:18:10Z" level=error msg="error syncing '_t webhook-ef770c7a-d6b/default': handler acorn-controller networking.k8s.io/v1, Kind=Ingress: failed to update existing owned object webhook-ef770c7a-d6b/acorn-webhook-default-api-5000-5000-5000 networking.k8s.io/v1, Kind=NetworkPolicy for acorn-controller webhook-ef770c7a-d6b/default, old subcontext [acorn-controller] gvk [networking.k8s.io/v1, Kind=Ingress] namespace [webhook-ef770c7a-d6b] name [default]: networkpolicies.networking.k8s.io \"acorn-webhook-default-api-5000-5000-5000\" already exists, requeuing"
```

In the underlying K8s cluster, we can see no Pods among *api*, *wss*, *api*, *www* were created:

```
$ k get po -A
NAMESPACE                             NAME                                      READY   STATUS      RESTARTS   AGE
kube-system                           coredns-59b4f5bbd5-z26mw                  1/1     Running     0          12d
kube-system                           local-path-provisioner-76d776f6f9-b2tcc   1/1     Running     0          12d
kube-system                           svclb-traefik-2d189581-qdxl2              2/2     Running     0          12d
kube-system                           helm-install-traefik-crd-qnp7k            0/1     Completed   0          12d
kube-system                           metrics-server-7b67f64457-6htj8           1/1     Running     0          12d
acorn-system                          acorn-controller-85cc65ccd4-qd2kx         1/1     Running     0          12d
acorn-system                          acorn-api-5568c69b9-496t6                 1/1     Running     0          12d
acorn-image-system                    containerd-config-path-fh7ts              1/1     Running     0          12d
acorn-image-system                    registry-59466fcd64-szk9s                 1/1     Running     0          12d
acorn-image-system                    buildkitd-76d44d54b-4mhrb                 2/2     Running     0          12d
kube-system                           traefik-68bbd8486b-brfpl                  1/1     Running     0          3h44m
kube-system                           helm-install-traefik-v6zml                0/1     Completed   0          86m
webhook-ef770c7a-d6b                  default-866c6b6856-sdcmd                  1/1     Running     0          7m22s
webhook-ef770c7a-d6b                  nats-7bc944c795-kt8kx                     1/1     Running     0          7m22s
webhook-atlas-bf6c221f-4c1f0001-5bf   create-mongo-atlas-service-l4qsm          0/1     Completed   0          7m21s
```

WIP: currently working on this issue to make sure the webhooks app can be deployed with an Atlas Mongo cluster :) Will update this demo soon